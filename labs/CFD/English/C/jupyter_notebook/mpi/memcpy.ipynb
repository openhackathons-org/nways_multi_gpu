{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Learning Objectives\n",
    "\n",
    "We will learn about the following in this lab:\n",
    "\n",
    "* Point-to-point and collective MPI communication routines.\n",
    "* Managing the two-level hierarchy created by global and local rank of a process and how it accesses GPU(s).\n",
    "* OpenMPI process mappings and its effect on application performance.\n",
    "\n",
    "**Note:** Execution results can vary significantly based on the MPI installation, supporting libraries, workload manager, and underlying CPU and GPU hardware configuration and topology. The codes in this lab have been tested on DGX-1 8 Tesla V100 16 GB nodes connected by Mellanox InfiniBand NICs running OpenMPI v4.1.1 with HPCX 2.8.1 and CUDA v11.3.0.0 as well as DGX with 8 Ampere A100 80GB nodes (OpenMPI v4.1.1, HPC SDK 22.7, HPCX 2.11).\n",
    "\n",
    "## MPI Inter-Process Communication\n",
    "\n",
    "Let us learn more about how MPI communicates between processes.\n",
    "\n",
    "### Point-to-Point communication\n",
    "\n",
    "Two MPI processes can communicate directly (point-to-point) by sending and receiving data packets to and from each other. Both the sender and receivers processes must acknowledge the transaction using `MPI_Send` and `MPI_Recv` functions. MPI allows tagging messages to differenciate between various messages that processes may send to each other.\n",
    "\n",
    "The function syntax for `MPI_Send` is:\n",
    "\n",
    "```c\n",
    "int MPI_Send(void* data, int count, MPI_Datatype datatype, int destination, \n",
    "         int tag, MPI_Comm communicator);\n",
    "```\n",
    "\n",
    "Similarly, the syntax for `MPI_Recv` is:\n",
    "\n",
    "```c\n",
    "int MPI_Recv(void* data, int count, MPI_Datatype datatype, int source, int tag,\n",
    "         MPI_Comm communicator, MPI_Status* status);\n",
    "```\n",
    "   \n",
    "A simple 2-process send-receive code is as follows:\n",
    "\n",
    "```c\n",
    "int data;\n",
    "if (rank == 0) {\n",
    "    data = -1;\n",
    "    MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "} else if (rank == 1) {\n",
    "    MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "}\n",
    "```\n",
    "\n",
    "There are several other functions to send and receive data synchronously and asynchronously. In particular, we will make use of `MPI_SendRecv` function which sends and receives a message, and whose syntax is as follows:\n",
    "\n",
    "```c\n",
    "int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "                int dest, int sendtag,\n",
    "                void *recvbuf, int recvcount, MPI_Datatype recvtype,\n",
    "                int source, int recvtag,\n",
    "                MPI_Comm comm, MPI_Status *status);\n",
    "```\n",
    "\n",
    "### Collective communication\n",
    "\n",
    "Collective communication involves participation of all processes in a communicator. It implies an implicit synchronization point among processes. Depending on the requirement, we can peform broadcast, scatter, gather, reduce, and other operations between the participating processes. \n",
    "\n",
    "In our application, we would like to reduce all the rank-local norms to a single global norm using the sum operation. We use the `MPI_Allreduce` function for it which combines and reduces values from all processes and distributes the result back to all processes, and whose syntax is as follows:\n",
    "\n",
    "```c\n",
    "int MPI_Allreduce(const void *sendbuf, void *recvbuf, int count,\n",
    "                  MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);\n",
    "```\n",
    "\n",
    "The `op` in our case will be `MPI_SUM`.\n",
    "\n",
    "## Communication Models\n",
    "\n",
    "We will use multiple ranks within our program as we will use multiple nodes. There are three major approaches to handle GPUs within a node:\n",
    "\n",
    "1. Single GPU per rank\n",
    "  * One process controls one GPU.\n",
    "  * Easier to program and understand.\n",
    "  * We can re-use our domain decomposition approach.\n",
    "\n",
    "\n",
    "2. Multiple GPUs per rank\n",
    "  * Usually, all GPUs within a node are handled by one process.\n",
    "  * Coordinating between GPUs is quite tricky as CUDA-based communication is intertwined with MPI communication.\n",
    "  * Requires a new decomposition for the two-tier communication hierarchy (MPI and CUDA).\n",
    "\n",
    "\n",
    "3. Single GPU per multiple ranks\n",
    "  * Multiple processes use the same GPU and number of processes in a node is usually equal to number of cores.\n",
    "  * Intended for heterogeneous codes where both CPU and GPU accelerate the application.\n",
    "  * CUDA Multi-Process-Service (MPS) is required to allow multiple CUDA processes to share a single GPU context.\n",
    "  \n",
    "We will take the first approach due to its simplicity (which eliminates approach #2) and because our application doesn't utilize CPU for compute (which eliminates approach #3). Thus our rank (core) to GPU mapping is one-to-one, as follows:\n",
    "\n",
    "![mpi_overview](../../images/mpi_overview.png)\n",
    "\n",
    "### Nodel-Level Local Rank\n",
    "\n",
    "As we will run on multiple nodes, for example 2 nodes, the number of processes launched will be 16 ( assuming 8 GPU per node like in DGX). This requires addional mapping of process id to GPU Device ID, which runs from 0 to 7 on each node. Thus, we need to create a local rank at the node level.\n",
    "\n",
    "To achieve this, we split the `MPI_COMM_WORLD` communicator between the nodes and store it in a `local_comm` communicator. Then, we get the local rank by calling the familiar `MPI_Comm_rank` function. Finally, we free the `local_comm` communicator as we don't require it anymore. \n",
    "\n",
    "The code snippet to obtain the `local_rank` at each node level is as follows:\n",
    "\n",
    "```c\n",
    "int local_rank = -1;\n",
    "MPI_Comm local_comm;\n",
    "MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &local_comm);\n",
    "MPI_Comm_rank(local_comm, &local_rank);\n",
    "MPI_Comm_free(&local_comm);\n",
    "```\n",
    "\n",
    "## Implementation Exercise: Part 1\n",
    "\n",
    "### Code Structure\n",
    "\n",
    "Open the [jacobi_memcpy_mpi.cpp](../../source_code/mpi/jacobi_memcpy_mpi.cpp) file and the [jacobi_kernels.cu](../../source_code/mpi/jacobi_kernels.cu) files. Alternatively, you can navigate to `CFD/English/C/source_code/mpi/` directory in Jupyter's file browser in the left pane. Then, click to open the `jacobi_memcpy_mpi.cpp` and `jacobi_kernels.cu` files.\n",
    "\n",
    "We separate the device kernels from other CUDA and MPI functions as `nvc++` compiler is required to compile CUDA C++ which may not be installed on some platforms Note that NVIDIA's HPC SDK includes the `nvc++` compiler.\n",
    "\n",
    "Review the [Makefile](../../source_code/mpi/Makefile) to see that we compile the CUDA kernels using `nvcc` and link the object file with `jacobi_memcpy_mpi.cpp` using `mpicxx` compiler as follows:\n",
    "\n",
    "```bash\n",
    "# Compiling jacobi_kernels.cu\n",
    "nvcc -gencode arch=compute_80,code=sm_80 -std=c++14 jacobi_kernels.cu -c\n",
    "# Compiling and linking with jacobi_cuda_aware_mpi.cpp\n",
    "mpicxx -I${CUDA_HOME}/include -fopenmp -std=c++14 jacobi_cuda_aware_mpi.cpp jacobi_kernels.o \\\n",
    "        -L${CUDA_HOME}/lib64 -lcudart -lnvToolsExt -o jacobi_cuda_aware_mpi\n",
    "```\n",
    "\n",
    "The device kernels are same as in previous labs. Open `jacobi_memcpy_mpi.cpp` file and understand the flow of the program. In particular, observe the following:\n",
    "\n",
    "1. `local_rank` is used to set the current GPU device.\n",
    "2. Device kernel calls have been replaced with function wrappers for ease of compilation.\n",
    "3. Rank 0 is used to calculate efficiency and other metrics, even though all ranks compute `single_gpu` function to verify multi-GPU implementation's correctness.\n",
    "4. In the first set of halo exchanges, `top_halo_buf` stores the top halo copied from the device on the host which is then sent to top neighbour. Whereas `bot_halo_buf` stores the updated bottom halo received from bottom neighbour that is then copied to the device from the host.\n",
    "5. In the second set of halo exchanges, `top_halo_buf` stores the updated top halo received from the top neighbour that is then copied to the device from the host. Whereas `bot_halo_buf` stores the bottom halo copied from the device to the host that is then sent to the bottom neighbour.\n",
    "6. Each halo exchange is wrapped in NVTX \"Halo exchange Memcpy+MPI\" for ease of viewing in profiler.\n",
    "\n",
    "### To-Do\n",
    "\n",
    "Now, implement the following marked as `TODO: Part 1-`:\n",
    "\n",
    "* Obtain the node-level local rank by splitting the global communicator.\n",
    "* Implement the MPI portion of first set of halo exchanges using `MPI_SendRecv` as explained above.\n",
    "* Implement the Memcpy operations and MPI calls for the second set of halo exchanges. Recall why `cudaMemcpyAsync` is not the correct way of implementing this MPI program.\n",
    "* Reduce the rank-local L2 Norm to a global L2 norm using `MPI_Allreduce` function.\n",
    "\n",
    "After implementing these, compile the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/mpi && make clean && make jacobi_memcpy_mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure there are no compilation errors. Now, let us validate the program. \n",
    "\n",
    "The grid-size of 16384$\\times$16384 will fully utilize all 8 GPUs. To test with 16 GPUs, we can increase the grid size to 16384$\\times$32768 to maintain the invariant that GPUs are not under-utilized. Note that the halo exchange copy size remains the same as before (16K elements * size of float (4B) = 64KB).\n",
    "\n",
    "**Due to limited resources, we will be using smaller grid ($2K\\times4K$) using 2 GPUs.**\n",
    "\n",
    "- **To run on half a node with 4 GPUs with $4K\\times8K$ grid size, use: `mpirun -np 4 --map-by ppr:4:socket ./jacobi_memcpy_mpi -nx 4096 -ny 8192.`**\n",
    "- **To run on full node with 8 GPUs with $16K\\times16K$ grid size, use: `mpirun -np 8 --map-by ppr:4:socket ./jacobi_memcpy_mpi -ny 16384.`**\n",
    "- **To run across 2 nodes with 16 GPUs with $16K\\times32K$ grid size, use: `mpirun -np 16 --map-by ppr:4:socket ./jacobi_memcpy_mpi -ny 32768` inside batch script or simply run `srun --partition=gpu  --nodes=2 --gres=gpu:8  --ntasks=16 --ntasks-per-node=8 --mpi=pmix --ntasks-per-socket=4 ./jacobi_memcpy_mpi -ny 32768` on the command line** ***<mark>NOTE: If resources are available</mark>***\n",
    "\n",
    "Run the program with 2 processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/mpi && srun --partition=gpu -n1 --gres=gpu:2  --ntasks=2 --ntasks-per-node=2 --mpi=pmix --ntasks-per-socket=2 ./jacobi_memcpy_mpi -nx 2048 -ny 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGX system with 8 Ampere A100\n",
    "Partial results obtained from a DGX system with 8 A100s:\n",
    "\n",
    "Using 2 GPUs (2K$\\times$4K grid size)\n",
    "```bash\n",
    "Num GPUs: 2.\n",
    "2048x4096: 1 GPU:   0.1366 s, 2 GPUs:   0.1258 s, speedup:     1.09, efficiency:    54.28 \n",
    "```\n",
    "\n",
    "Using 4 GPUs, half a node (4K$\\times$8K grid size)\n",
    "```bash\n",
    "Num GPUs: 4.\n",
    "4096x8192: 1 GPU:   0.4438 s, 4 GPUs:   0.1889 s, speedup:     2.35, efficiency:    58.74 \n",
    "```\n",
    "\n",
    "Using 8 GPUs, full node (16384$\\times$16384 grid size)\n",
    "```bash\n",
    "Num GPUs: 8.\n",
    "16384x16384: 1 GPU:   3.3022 s, 8 GPUs:   0.6327 s, speedup:     5.22, efficiency:    65.24 \n",
    "```\n",
    "\n",
    "Using 2 nodes connected by InfiniBand (IB) NICs (16384$\\times$32768 grid size)\n",
    "```bash\n",
    "Num GPUs: 16.\n",
    "16384x32768: 1 GPU:   6.5526 s, 16 GPUs:   0.6500 s, speedup:    10.08, efficiency:    63.01 \n",
    "```\n",
    "\n",
    "### DGX system with 8 Tesla V100\n",
    "Partial results obtained from a DGX system with 8 Tesla V100s:\n",
    "\n",
    "Using 2 nodes connected by InfiniBand (IB) NICs (16384$\\times$32768 grid size):\n",
    "\n",
    "```bash\n",
    "Num GPUs: 16.\n",
    "16384x32768: 1 GPU:   8.9057 s, 16 GPUs:   0.7695 s, speedup:    11.57, efficiency:    72.34 \n",
    "```\n",
    "\n",
    "Using 4 nodes connected by InfiniBand (IB) NICs (16K$\\times$64K grid size, $4\\times$ the single-node's grid size):\n",
    "```bash\n",
    "Num GPUs: 32.\n",
    "16384x65536: 1 GPU:  17.6316 s, 32 GPUs:   0.8526 s, speedup:    20.68, efficiency:    64.62\n",
    "```\n",
    "\n",
    "As the communication overhead increases due to more inter-node communication, the speed-up obtained and thus the efficiency of the application decreases. Nonetheless, our program can scale across mutliple nodes.\n",
    "\n",
    "### OpenMPI Process Mappings\n",
    "\n",
    "As we mentioned in previous labs, there are multiple ways to specify the number of processes to be run on each socket, node, etc. One such way is to use `--map-by` option. Mapping assigns a default location to each process.  To specify that we want each socket to run 4 processes, we use `--map-by ppr:4:socket` flag. Here, `ppr` stands for processes-per-resource, where the spcified resource is `socket` and the spcified number of processes is `4`. It is similar to using the `-npersocket 4` option. \n",
    "\n",
    "<mark>When launching tasks via slurm, we use `--ntasks-per-socket` instead of `-npersocket` to specify the number of tasks to invoke on each socket and we use `--ntasks-per-socket`, instead of `--map-by ppr:4:socket` to specify number of tasks per socket.</mark> Feel free to review the list of common [Slurm flags](https://slurm.schedmd.com/mc_support.html#flags).\n",
    "\n",
    "If using `mpirun` inside batch script, you can verify this by using `mpirun -np 16 --map-by ppr:4:socket ./jacobi_memcpy_mpi -ny 32768` to run the executable across 2 nodes (8 tasks per node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `--map-by ppr:8:node:4:socket` flag with `mpirun`. This allows us to specify the number of processes per socket as well as the number of processes per node. This should result in the same execution and results. \n",
    "\n",
    "If using `mpirun` inside batch script, you can verify this by using `mpirun -np 16 --map-by ppr:8:node:4:socket ./jacobi_memcpy_mpi -ny 32768` to run the executable across 2 nodes (8 tasks per node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example commands to run the executable using `srun` on multinode (if recourses permit)\n",
    "\n",
    "- Without mapping tasks to node and socket: `srun --partition=gpu  --nodes=2 --gres=gpu:8  --ntasks=16 --mpi=pmix ./jacobi_memcpy_mpi -ny 32768`\n",
    "- With mapping tasks to node and socket: `srun --partition=gpu  --nodes=2 --gres=gpu:8  --ntasks=16 --mpi=pmix --ntasks-per-socket=4 --ntasks-per-node=8 ./jacobi_memcpy_mpi -ny 32768`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we re-run the executable with and without the `--map-by ppr:8:node:4:socket` flag and compare the results, we notice the increase in multi-node execution time and corresponding decrease in efficiency. Check the below partial results obtained for $16K\\times32K$ grid size over 2 nodes:\n",
    "\n",
    "### DGX system with 8 Ampere A100\n",
    "Partial results obtained from a DGX system with 8 A100s using 2 nodes connected by InfiniBand (IB) NICs (16384$\\times$32768 grid size):\n",
    "\n",
    "```bash\n",
    "# with mapping tasks to nodes and socket\n",
    "Num GPUs: 16.\n",
    "16384x32768: 1 GPU:   6.5668 s, 16 GPUs:   0.6528 s, speedup:    10.06, efficiency:    62.87 \n",
    "```\n",
    "\n",
    "```bash\n",
    "# without mapping tasks to nodes and socket\n",
    "Num GPUs: 16.\n",
    "16384x32768: 1 GPU:  15.4872 s, 16 GPUs:   1.4351 s, speedup:    10.79, efficiency:    67.45 \n",
    "```\n",
    "\n",
    "### DGX system with 8 Tesla V100\n",
    "Partial results obtained from a DGX system with 8 Tesla V100s using 2 nodes connected by InfiniBand (IB) NICs (16384$\\times$32768 grid size):\n",
    "\n",
    "```bash\n",
    "# with mapping tasks to nodes and socket\n",
    "Num GPUs: 16.\n",
    "16384x32768: 1 GPU:   8.9050 s, 16 GPUs:   0.8150 s, speedup:    10.93, efficiency:    68.2\n",
    "```\n",
    "\n",
    "```bash\n",
    "# without mapping tasks to nodes and socket\n",
    "Num GPUs: 16.\n",
    "16384x32768: 1 GPU:   8.9057 s, 16 GPUs:   0.7695 s, speedup:    11.57, efficiency:    72.34 \n",
    "```\n",
    "\n",
    "Let us check what cores or sockets or nodes each process (or MPI rank) is bound to. Binding constrains each process to run on specific processors. We use the `--report-bindings` option to check this. \n",
    "\n",
    "**NOTE:** To check the bindings, one can simply use `mpirun -np 16 --map-by ppr:8:node:4:socket --report-bindings ./jacobi_memcpy_mpi -ny 32768` inside batch script to run the executable on 2 nodes (if resources are available) or use `salloc` to allocate resources and then run `mpirun`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we check the bindings, the output may seem cluttered. Let's look at the partial output from ranks 0 and 1:\n",
    "\n",
    "```bash\n",
    "[<node_0_name>:<proc_id>] MCW rank 0 bound to socket 0 ... [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../..]\n",
    "[<node_0_name>:<proc_id>] MCW rank 1 bound to socket 1 ... [../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]\n",
    "```\n",
    "\n",
    "Rank 0 is bound to all cores on socket 0 on node 0 while rank 1 is bound to all cores on socket 1 on node 0. \n",
    "\n",
    "\n",
    "Partial example results obtained from a DGX system with 8 A100s using 1 node (using `mpirun -np 16 --map-by ppr:8:node:4:socket --report-bindings ./jacobi_memcpy_mpi -ny 32768` for 16384$\\times$32768 grid size:\n",
    "\n",
    "```bash\n",
    "[dgx02:2451349] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2451349] MCW rank 1 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx02:2451349] MCW rank 2 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2451349] MCW rank 3 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx02:2451349] MCW rank 4 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2451349] MCW rank 5 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx02:2451349] MCW rank 6 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2451349] MCW rank 7 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "\n",
    "[dgx03:3172932] MCW rank 8 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx03:3172932] MCW rank 9 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx03:3172932] MCW rank 10 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx03:3172932] MCW rank 11 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx03:3172932] MCW rank 12 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx03:3172932] MCW rank 13 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx03:3172932] MCW rank 14 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx03:3172932] MCW rank 15 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "```\n",
    "\n",
    "Clearly, this is not an optimal arrangement as halo exchanges have to cross socket boundaries for process. Now, if we check the process bindings in the previous case, we can see that ranks 0 and 1 are bound to the same socket in the same node. Moreover, ranks 3 and 4 are bound to different sockets (as `<procs_per_socket>` is 4) but bound to the same node, as desired.\n",
    "\n",
    "\n",
    "Partial example results obtained from a DGX system with 8 A100s using 1 node (using `mpirun -np 16 --map-by ppr:4:socket --report-bindings ./jacobi_memcpy_mpi -ny 32768` for 16384$\\times$32768 grid size:\n",
    "\n",
    "```bash\n",
    "\n",
    "[dgx01:3961004] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx01:3961004] MCW rank 1 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx01:3961004] MCW rank 2 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx01:3961004] MCW rank 3 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx01:3961004] MCW rank 4 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx01:3961004] MCW rank 5 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx01:3961004] MCW rank 6 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx01:3961004] MCW rank 7 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx02:2165925] MCW rank 8 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2165925] MCW rank 9 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2165925] MCW rank 10 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2165925] MCW rank 11 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]]: [BB/BB/BB/BB][../../../..]\n",
    "[dgx02:2165925] MCW rank 12 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx02:2165925] MCW rank 13 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx02:2165925] MCW rank 14 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "[dgx02:2165925] MCW rank 15 bound to socket 1[core 4[hwt 0-1]], socket 1[core 5[hwt 0-1]], socket 1[core 6[hwt 0-1]], socket 1[core 7[hwt 0-1]]: [../../../..][BB/BB/BB/BB]\n",
    "```\n",
    "\n",
    "It is quite easy to end up in a sub-optimal process mapping by using simple OpenMPI flags and options. Thus, it is always advisible to double-check the process-to-core and process-to-socket bindings. Moving forward, we will use the `--map-by ppr:4:socket` option as evidently it results in desired process-to-core, socket, and node mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling\n",
    "\n",
    "We can profile an MPI program in two ways. To profile everything, putting the data in one file:\n",
    "\n",
    "`nsys [nsys options] mpirun [mpi options] <program>`\n",
    "\n",
    "To profile everything putting the data from each rank into a separate file:\n",
    "\n",
    "`mpirun [mpi options] nsys profile [nsys options] <program>`\n",
    "\n",
    "We will use the latter approach as it produces a single report and is more convenient to view. The host compute nodes need a working installation of Nsight Systems.\n",
    "\n",
    "Let's profile the application using `nsys` (on 4 GPUs): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../source_code/mpi && sbatch profiling_4g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the profiler report, you would need to Download and save the report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](../../source_code/mpi/output_profiler/jacobi_memcpy_mpi_report.nsys-rep) and choosing Save Link As. Once done open the report via the GUI.\n",
    "\n",
    "<!--You may notice that only 8 MPI processes are visible even though we launched 16 MPI processes. Nsight Systems displays the output from a single node and inter-node transactions (copy operations) are visible. This is for ease of viewing and doesn't impede our analysis.-->\n",
    "\n",
    "Below is an example report (using 2 nodes):\n",
    "\n",
    "![mpi_memcpy_overview](../../images/mpi_memcpy_overview.png)\n",
    "\n",
    "Observe the following in the Timeline snapshot:\n",
    "\n",
    "* Two sets of halo exchanges take place, each consisting of DtoH and HtoD CUDA Memcpy with an `MPI_Sendrecv` call in between for inter-process communication followed by an `MPI_Allreduce` call. \n",
    "* Each halo exchange takes about $45\\mu$s in hardware and about $60\\mu$s overall including the software overhead.\n",
    "* The time between two Jacobi kernel iterations is about $200\\mu$s.\n",
    "\n",
    "However, if you scroll back in time, you might notice that not all halo exchanges take $60\\mu$s. For example, here's a snapshot from near the beginning of the multi-GPU Jacobi iteration loop:\n",
    "\n",
    "![mpi_memcpy_large_time](../../images/mpi_memcpy_large_time.png)\n",
    "\n",
    "Here, the halo exchange takes about $1100\\mu$s. MPI uses a lot of heuristics to fine-tune its call-stack and communication protocol to enhance performance. Therefore, we observe the behavior shown above where initially MPI calls take significant time but it improves in subsequent iterations.\n",
    "\n",
    "**Solution:** The solution for this exercise is present in `source_code/mpi/solutions` directory: [jacobi_memcpy_mpi.cpp](../../source_code/mpi/solutions/jacobi_memcpy_mpi.cpp).\n",
    "\n",
    "Note that our current implementation uses explicit host-staging for every halo copy operation. From our previous labs, we know that within a node, GPU-to-GPU communication can bypass host-staging and we implemented it using DtoD CUDA Memcpy with P2P enabled. Certainly, eliminating host-staging should improve performance. There are also inter-node communication optimizations that we can employ. \n",
    "\n",
    "We will learn more about both intra-node and inter-node GPU-centric MPI communication optimizations in the next lab where we will work with CUDA-aware MPI. Click below to move to the next lab:\n",
    "\n",
    "# [Next: CUDA-aware MPI](../mpi/cuda_aware.ipynb)\n",
    "\n",
    "Here's a link to the home notebook through which all other notebooks are accessible:\n",
    "\n",
    "# [HOME](../../../start_here.ipynb)\n",
    "\n",
    "---\n",
    "## Links and Resources\n",
    "\n",
    "* [Programming Concepts: MPI Point-to-Point Communication](https://cvw.cac.cornell.edu/mpip2p/p2pdef)\n",
    "* [Programming Concepts: MPI Collective Communication](https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture29.pdf)\n",
    "* [Programming Concepts: NVIDIA Multi-Process Service](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)\n",
    "* [Documentation: MPI Processing Mapping, Ranking, and Binding](https://www.open-mpi.org/doc/current/man1/mpirun.1.php#sect12)\n",
    "* [Code: Multi-GPU Programming Models](https://github.com/NVIDIA/multi-gpu-programming-models)\n",
    "* [Code: GPU Bootcamp](https://github.com/gpuhackathons-org/gpubootcamp/)\n",
    "\n",
    "Don't forget to check out additional [Open Hackathons Resources](https://www.openhackathons.org/s/technical-resources) and join our [OpenACC and Hackathons Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "## Licensing\n",
    "Copyright © 2022 OpenACC-Standard.org.  This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
